# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

The 'bankmarketing_train' dataset contains data attributes for customers which are meant to assist in predicting whether or not they are likley to subscribe to a term deposit or not, the outcome variable we are intersted in is labelled as 'y'.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best performing model was obtained by the AutoML approach which identified MaxAbsScaler LightGBM as the best mode with a score of 0.9151.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The hyperparameter tuning was more manual and detailed oriented with the configurations that had to be set. For example the process to appropriatley submit my hyperdrive run to the experiment involved defining multiple variables across multiple classes such as PrimaryMetricGoal, BanditPolicy, RandomParameterSampling, and HyperDriveConfig. In these variables of conigutations, specifically in the ScriptRunConfig, you specify the path of where the tain.py script could be obtained which held the code to source the data.

**What are the benefits of the parameter sampler you chose?**

The benefits of paramter sampleing is that it allows us to sample values from a predefined range, so with a single run you are obtain to obtain the analaysis and trial for a range of parameters.


**What are the benefits of the early stopping policy you chose?**

One parameter that I can expand upon regarding the configuring of the early stopping policy is the 'slack_factor'. In my code I set up the slack_factor to be 0.1 which means that the run's performance must be within 10% of the best performing run to continue. This is useuful because it provides us the flexibility to declare what boundaries we have for what is considered an under performing run.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

In our AutoML run we specified a few hyperparameters such as the task (whcih was classification), n_cross_validations, and the primary metric. In searching through a wide array of models and comparing then against each other in measure MaxAbsScaler LightGBM to be the best with a score of 0.9151.


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

The AutoML approach determined the best socred model. the accuracy difference that I obtained was not significantly difference, for hyperdrive I got a score of 0.908649468892261 and for AutoML it was 0.9151. The main difference in architecture and approach is that hyperdrive allows for a much more customizable experience, but the benefit of AutoML is that it searches through a wider range of models and their parameters. In our case, AutoML was able to pick a model that performed marginally better.


## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

One improvement that can be done for this experiment is to utilize hyperdrive and AutoML to support one another. The barrier of entry of AutoML is much easier with its parameter set up and we are able to search through a wide array of model. Once we do that, we can take which ever model was determined best and go use hyperdrive to customer the parameters are a more granular level to try and improve the score in order to be more efficient in how we predict our target variable for a classification type of dataset.

